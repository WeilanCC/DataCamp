{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Paris Saclay Center for Data Science](http://www.datascience-paris-saclay.fr)\n",
    "\n",
    "## [Fake news RAMP](http://www.ramp.studio/problems/fake_news): classify statements of public figures\n",
    "\n",
    "_Emanuela Boros (LIMSI/CNRS), Balázs Kégl (LAL/CNRS), Roman Yurchak (Symerio)_\n",
    "\n",
    "## Introduction\n",
    "This is an initiation project to introduce RAMP and get you to know how it works.\n",
    "\n",
    "The goal is to develop prediction models able to **identify which news is fake**. \n",
    "\n",
    "The data we will manipulate is from http://www.politifact.com. The input contains of short statements of public figures (and sometimes anonymous bloggers), plus some metadata. The output is a truth level, judged by journalists at Politifact. They use six truth levels which we coded into integers to obtain an [ordinal regression](https://en.wikipedia.org/wiki/Ordinal_regression) problem:\n",
    "```\n",
    "0: 'Pants on Fire!'\n",
    "1: 'False'\n",
    "2: 'Mostly False'\n",
    "3: 'Half-True'\n",
    "4: 'Mostly True'\n",
    "5: 'True'\n",
    "```\n",
    "You goal is to classify each statement (+ metadata) into one of the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "* numpy>=1.10.0  \n",
    "* matplotlib>=1.5.0 \n",
    "* pandas>=0.19.0  \n",
    "* scikit-learn>=0.17 (different syntaxes for v0.17 and v0.18)   \n",
    "* seaborn>=0.7.1\n",
    "* nltk\n",
    "\n",
    "Further, an nltk dataset needs to be downloaded:\n",
    "\n",
    "```\n",
    "python -m nltk.downloader popular\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filename = 'data/train.csv'\n",
    "data = pd.read_csv(train_filename, sep='\\t')\n",
    "data['date'] = pd.to_datetime(data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original training data frame has 13000+ instances. In the starting kit, we give you a subset of 7569 instances for training and 2891 instances for testing.\n",
    "\n",
    "Most columns are categorical, some have high cardinalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(data['state']))\n",
    "print(len(np.unique(data['state'])))\n",
    "data.groupby('state').count()[['job']].sort_values(\n",
    "    'job', ascending=False).reset_index().rename(\n",
    "    columns={'job': 'count'}).plot.bar(\n",
    "    x='state', y='count', figsize=(16, 10), fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(data['job']))\n",
    "print(len(np.unique(data['job'])))\n",
    "data.groupby('job').count()[['state']].rename(\n",
    "    columns={'state': 'count'}).sort_values(\n",
    "    'count', ascending=False).reset_index().plot.bar(\n",
    "        x='job', y='count', figsize=(16, 10), fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the journalist and the editor as input, you will need to split the lists since sometimes there are more than one of them on an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(data['edited_by']))\n",
    "print(len(np.unique(data['edited_by'])))\n",
    "data.groupby('edited_by').count()[['state']].rename(\n",
    "    columns={'state': 'count'}).sort_values(\n",
    "    'count', ascending=False).reset_index().plot.bar(\n",
    "        x='edited_by', y='count', figsize=(16, 10), fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(data['researched_by']))\n",
    "print(len(np.unique(data['researched_by'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('researched_by').count()[['state']].sort_values(\n",
    "    'state', ascending=False).reset_index().rename(\n",
    "    columns={'state': 'count'}).plot.bar(\n",
    "        x='researched_by', y='count', figsize=(16, 10), fontsize=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2000+ different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(data['source']))\n",
    "print(len(np.unique(data['source'])))\n",
    "data.groupby('source').count()[['state']].rename(\n",
    "    columns={'state': 'count'}).sort_values(\n",
    "    'count', ascending=False).reset_index().loc[:100].plot.bar(\n",
    "        x='source', y='count', figsize=(16, 10), fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting truth level\n",
    "\n",
    "The goal is to predict the truthfulness of statements. Let us group the data according to the `truth` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('truth').count()[['source']].reset_index().plot.bar(x='truth', y='source');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline\n",
    "\n",
    "For submitting at the [RAMP site](http://ramp.studio), you will have to write two classes, saved in two different files:   \n",
    "* the class `FeatureExtractor`, which will be used to extract features for classification from the dataset and produce a numpy array of size (number of samples $\\times$ number of features). \n",
    "* a class `Classifier` to predict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction overview\n",
    "\n",
    "Before going through the code, we first need to understand how **tf-idf** works. A **Term Frequency** is a count of how many times a word occurs in a given document (synonymous with bag of words). The **Inverse Document Frequency** is the number of times a word occurs in a corpus of documents. **tf-idf** is used to weight words according to how important they are. Words that are used frequently in many documents will have a lower weighting while infrequent ones will have a higher weighting.\n",
    "\n",
    "\n",
    "The ``FeatureExtractor`` class is used to extract features\n",
    "from text documents. It is based on the [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) class from scikit-learn which is a [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) followed by [`TfidfTransformer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer).\n",
    "\n",
    "See the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) for a general introduction to text feature extraction.\n",
    "\n",
    "`CountVectorizer` converts a collection of text documents to a matrix of token (*word*) counts. This implementation produces a sparse representation of the counts to be passed to the `TfidfTransformer`.\n",
    "The `TfidfTransformer` transforms a count matrix to a normalized tf or tf-idf representation.\n",
    "\n",
    "A `TfidfVectorizer` does these two steps. \n",
    "\n",
    "The feature extractor overrides *fit* by providing the `TfidfVectorizer` with a new preprocessing step that is presented after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving  feature extraction\n",
    "\n",
    "#### Preprocessing \n",
    "\n",
    "The document preprocessing can be customized in the `document_preprocessor` function.\n",
    "\n",
    "For instance, to transform accentuated unicode symbols into their simple counterpart e.g. è -> e, the following function can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "subjects_threshold = 250\n",
    "job_threshold = 100\n",
    "source_threshold = 50\n",
    "state_threshold = 100\n",
    "\n",
    "subjects = [ast.literal_eval(data['subjects'][i]) for i in range(data.shape[0])]\n",
    "subjects = dict(Counter(itertools.chain.from_iterable(subjects)))\n",
    "subjects = {k:v for k,v in zip(subjects.keys(), subjects.values()) if v > subjects_threshold}\n",
    "subjects['Other subject'] = 1\n",
    "subjects = sorted(set(subjects))\n",
    "print(subjects, \"\\n\")\n",
    "\n",
    "job = dict(Counter(list(data['job'].values)))\n",
    "del job['']\n",
    "del job['None']\n",
    "job = {k:v for k,v in zip(job.keys(), job.values()) if v > job_threshold}\n",
    "job['Other job'] = 1\n",
    "job = sorted(set(job))\n",
    "print(job, \"\\n\")\n",
    "\n",
    "source = dict(Counter(list(data['source'].values)))\n",
    "source = {k:v for k,v in zip(source.keys(), source.values()) if v > source_threshold}\n",
    "source['Other source'] = 1\n",
    "source = sorted(set(source))\n",
    "print(source, \"\\n\")\n",
    "\n",
    "state = dict(Counter(list(data['state'].values)))\n",
    "del state['']\n",
    "state = {k:v for k,v in zip(state.keys(), state.values()) if v > state_threshold}\n",
    "state['Other state'] = 1\n",
    "state = sorted(set(state))\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def document_preprocessor(doc):\n",
    "    # TODO: is there a way to avoid these encode/decode calls?\n",
    "    try:\n",
    "        doc = unicode(doc, 'utf-8')\n",
    "    except NameError:  # unicode is a default on python 3\n",
    "        pass\n",
    "    doc = unicodedata.normalize('NFD', doc)\n",
    "    doc = doc.encode('ascii', 'ignore')\n",
    "    doc = doc.decode(\"utf-8\")\n",
    "    return str(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see also the `strip_accents` option of [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "\n",
    "##### Stopword removal\n",
    "The most frequent words often do not carry much meaning. Examples: *the, a, of, for, in, ...*. \n",
    "\n",
    "Stop words removal can be enabled by passing the `stopwords='english'` parameter at the initialization of the\n",
    "[`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). \n",
    "\n",
    "A custom list of stop words (e.g. from NLTK) can also be used.\n",
    "\n",
    "##### Word / character n-grams\n",
    "\n",
    "By default, the bag of words model is use in the starting kit. To use word or character n-grams, the `analyser` and `ngram_range` parameters of [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) should be changed.\n",
    "\n",
    "\n",
    "##### Stemming  and Lemmatization\n",
    "\n",
    "English words like *look* can be inflected with a morphological suffix to produce *looks, looking, looked*. They share the same stem *look*. Often (but not always) it is beneficial to map all inflected forms into the stem. The most commonly used stemmer is the Porter Stemmer. The name comes from its developer, Martin Porter. `SnowballStemmer('english')` from *NLTK* is used. This stemmer is called Snowball, because Porter created a programming language with this name for creating new stemming algorithms.\n",
    "\n",
    "Stemming can be enabled with a custom `token_processor` function, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def token_processor(tokens):\n",
    "    for token in tokens:\n",
    "        yield stemmer.stem(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extractor\n",
    "\n",
    "The feature extractor implements a `transform` function. It is saved in the file [`submissions/pawel_guzewicz/feature_extractor.py`](/edit/submissions/pawel_guzewicz/feature_extractor.py). It receives the pandas dataframe `X_df` defined at the beginning of the notebook. It should produce a numpy array representing the extracted features, which will then be used for the classification.  \n",
    "\n",
    "**Note:** the following code cells are *not* executed in the notebook. The notebook saves their contents in the file specified in the first line of the cell, so you can edit your submission before running the local test below and submitting it at the RAMP site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file submissions/pawel_guzewicz/feature_extractor.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import ast\n",
    "\n",
    "def document_preprocessor(doc):\n",
    "    \"\"\" A custom document preprocessor\n",
    "\n",
    "    This function can be edited to add some additional\n",
    "    transformation on the documents prior to tokenization.\n",
    "\n",
    "    At present, this function passes the document through\n",
    "    without modification.\n",
    "    \"\"\"\n",
    "\n",
    "    return doc\n",
    "\n",
    "def token_processor(tokens):\n",
    "    \"\"\" A custom token processor\n",
    "    \n",
    "    This function can be edited to add some additional\n",
    "    transformation on the extracted tokens (e.g. stemming)\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    for token in tokens:\n",
    "        yield stemmer.stem(token)\n",
    "\n",
    "class FeatureExtractor(TfidfVectorizer):\n",
    "    \"\"\"Convert a collection of raw docs to a matrix of TF-IDF features. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        nltk_stop_words = set(stopwords.words('english'))\n",
    "        sklearn_stop_words = set(stop_words.ENGLISH_STOP_WORDS)\n",
    "        another_stop_words = set(['a', 'able', 'about', 'above', 'abroad', 'according', 'accordingly', 'across', 'actually', 'adj', 'after', 'afterwards', 'again', 'against', 'ago', 'ahead', 'ain\\'t', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'alongside', 'already', 'also', 'although', 'always', 'am', 'amid', 'amidst', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', 'aren\\'t', 'around', 'as', 'a\\'s', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'back', 'backward', 'backwards', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'begin', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', 'came', 'can', 'cannot', 'cant', 'can\\'t', 'caption', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'c\\'mon', 'co', 'co.', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', 'couldn\\'t', 'course', 'c\\'s', 'currently', 'd', 'dare', 'daren\\'t', 'definitely', 'described', 'despite', 'did', 'didn\\'t', 'different', 'directly', 'do', 'does', 'doesn\\'t', 'doing', 'done', 'don\\'t', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'eighty', 'either', 'else', 'elsewhere', 'end', 'ending', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'evermore', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'fairly', 'far', 'farther', 'few', 'fewer', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forever', 'former', 'formerly', 'forth', 'forward', 'found', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', 'hadn\\'t', 'half', 'happens', 'hardly', 'has', 'hasn\\'t', 'have', 'haven\\'t', 'having', 'he', 'he\\'d', 'he\\'ll', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'here\\'s', 'hereupon', 'hers', 'herself', 'he\\'s', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'hundred', 'i', 'i\\'d', 'ie', 'if', 'ignored', 'i\\'ll', 'i\\'m', 'immediate', 'in', 'inasmuch', 'inc', 'inc.', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'inside', 'insofar', 'instead', 'into', 'inward', 'is', 'isn\\'t', 'it', 'it\\'d', 'it\\'ll', 'its', 'it\\'s', 'itself', 'i\\'ve', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'known', 'knows', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'let\\'s', 'like', 'liked', 'likely', 'likewise', 'little', 'll', 'look', 'looking', 'looks', 'low', 'lower', 'ltd', 'm', 'made', 'mainly', 'make', 'makes', 'many', 'may', 'maybe', 'mayn\\'t', 'me', 'mean', 'meantime', 'meanwhile', 'merely', 'might', 'mightn\\'t', 'mine', 'minus', 'miss', 'more', 'moreover', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'mustn\\'t', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needn\\'t', 'needs', 'neither', 'never', 'neverf', 'neverless', 'nevertheless', 'new', 'next', 'nine', 'ninety', 'no', 'nobody', 'non', 'none', 'nonetheless', 'noone', 'no-one', 'nor', 'normally', 'not', 'nothing', 'notwithstanding', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'one\\'s', 'only', 'onto', 'opposite', 'or', 'other', 'others', 'otherwise', 'ought', 'oughtn\\'t', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'past', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provided', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'recent', 'recently', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 'round', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'shan\\'t', 'she', 'she\\'d', 'she\\'ll', 'she\\'s', 'should', 'shouldn\\'t', 'since', 'six', 'so', 'some', 'somebody', 'someday', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', 'take', 'taken', 'taking', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', 'that\\'ll', 'thats', 'that\\'s', 'that\\'ve', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'there\\'d', 'therefore', 'therein', 'there\\'ll', 'there\\'re', 'theres', 'there\\'s', 'thereupon', 'there\\'ve', 'these', 'they', 'they\\'d', 'they\\'ll', 'they\\'re', 'they\\'ve', 'thing', 'things', 'think', 'third', 'thirty', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'till', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 't\\'s', 'twice', 'two', 'u', 'un', 'under', 'underneath', 'undoing', 'unfortunately', 'unless', 'unlike', 'unlikely', 'until', 'unto', 'up', 'upon', 'upwards', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 've', 'versus', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', 'wasn\\'t', 'way', 'we', 'we\\'d', 'welcome', 'well', 'we\\'ll', 'went', 'were', 'we\\'re', 'weren\\'t', 'we\\'ve', 'what', 'whatever', 'what\\'ll', 'what\\'s', 'what\\'ve', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'where\\'s', 'whereupon', 'wherever', 'whether', 'which', 'whichever', 'while', 'whilst', 'whither', 'who', 'who\\'d', 'whoever', 'whole', 'who\\'ll', 'whom', 'whomever', 'who\\'s', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', 'won\\'t', 'would', 'wouldn\\'t', 'x', 'y', 'yes', 'yet', 'you', 'you\\'d', 'you\\'ll', 'your', 'you\\'re', 'yours', 'yourself', 'yourselves', 'you\\'ve', 'z', 'zero'])\n",
    "        all_stop_words = list(nltk_stop_words | sklearn_stop_words | another_stop_words)\n",
    "        \n",
    "        super(FeatureExtractor, self).__init__(preprocessor=document_preprocessor, analyzer='word', lowercase=True, strip_accents='unicode', stop_words=all_stop_words)#, max_df=1, min_df=0.01)\n",
    "\n",
    "    def fit(self, X_df, y=None):\n",
    "        \"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_df : pandas.DataFrame\n",
    "            a DataFrame, where the text data is stored in the ``statement``\n",
    "            column.\n",
    "        \"\"\"\n",
    "\n",
    "        super(FeatureExtractor, self).fit(X_df.statement, y)\n",
    "        return self\n",
    "\n",
    "    def build_tokenizer(self):\n",
    "        \"\"\"\n",
    "        Internal function, needed to plug-in the token processor, cf.\n",
    "        http://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes\n",
    "        \"\"\"\n",
    "\n",
    "        tokenize = super(FeatureExtractor, self).build_tokenizer()\n",
    "        return lambda doc: list(token_processor(tokenize(doc)))\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        self.df = X_df[['job', 'source', 'state', 'subjects']]\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        subjects = ['Candidate Biography', 'Crime', 'Economy', 'Education', 'Elections', 'Energy', 'Environment', 'Federal Budget', 'Health Care', 'Immigration', 'Jobs', 'Message Machine 2010', 'Message Machine 2012', 'Other subject', 'State Budget', 'Taxes'] \n",
    "        job = ['Democrat', 'Other job', 'Republican']\n",
    "        source = ['Barack Obama', 'Chain email', 'Chris Christie', 'Hillary Clinton', 'Joe Biden', 'John Boehner', 'John McCain', 'Marco Rubio', 'Michele Bachmann', 'Mitt Romney', 'Newt Gingrich', 'Other source', 'Rick Perry', 'Rick Scott', 'Sarah Palin', 'Scott Walker'] \n",
    "        state = ['Arizona', 'Florida', 'Georgia', 'Illinois', 'Massachusetts', 'New Jersey', 'New York', 'Ohio', 'Oregon', 'Other state', 'Rhode Island', 'Texas', 'Virginia', 'Wisconsin']\n",
    "        self.df = self.df.join(pd.DataFrame(columns=subjects)).join(pd.DataFrame(columns=job)).join(pd.DataFrame(columns=source)).join(pd.DataFrame(columns=state))\n",
    "        self.df.fillna(0, inplace=True)\n",
    "        \n",
    "        for i in range(self.df.shape[0]):\n",
    "            subjects_row = ast.literal_eval(self.df['subjects'][i])\n",
    "            for j in subjects_row:\n",
    "                if j in subjects:\n",
    "                    self.df.at[i, j] = 1\n",
    "                else:\n",
    "                    self.df.at[1, 'Other subject'] = 1\n",
    "\n",
    "            job_ = self.df.at[i, 'job']\n",
    "            if str(job_) in job:\n",
    "                self.df.at[i, job_] = 1\n",
    "            else:\n",
    "                self.df.at[i, 'Other job'] = 1\n",
    "            \n",
    "            source_ = self.df.at[i, 'source']\n",
    "            if source_ in source:\n",
    "                self.df.at[i, source_] = 1\n",
    "            else:\n",
    "                self.df.at[i, 'Other source'] = 1\n",
    "            \n",
    "            state_ = self.df.at[i, 'state']\n",
    "            if state_ in state:\n",
    "                self.df.at[i, state_] = 1\n",
    "            else:\n",
    "                self.df.at[i, 'Other state'] = 1\n",
    "        \n",
    "        self.df.drop('subjects', axis=1, inplace=True)\n",
    "        self.df.drop('job', axis=1, inplace=True)\n",
    "        self.df.drop('source', axis=1, inplace=True)\n",
    "        self.df.drop('state', axis=1, inplace=True)\n",
    "\n",
    "        X = hstack([super(FeatureExtractor, self).transform(X_df.statement), scipy.sparse.csr_matrix(self.df.values)]).toarray()\n",
    "        #X = preprocessing.MaxAbsScaler().fit_transform(X)\n",
    "        #X = decomposition.IncrementalPCA(n_components=3).fit_transform(X)\n",
    "        #X = preprocessing.scale(X)\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X_df, y=None):\n",
    "        return self.fit(X_df, y).transform(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "\n",
    "The classifier follows a classical scikit-learn classifier template. It should be saved in the file [`submissions/pawel_guzewicz/classifier.py`](/edit/submissions/pawel_guzewicz/classifier.py). In its simplest form it takes a scikit-learn pipeline, assigns it to `self.clf` in `__init__`, then calls its `fit` and `predict_proba` functions in the corresponding member functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submissions/pawel_guzewicz/classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%file submissions/pawel_guzewicz/classifier.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from math import floor\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "class OneVsOneClassifierFixed(OneVsOneClassifier):\n",
    "    def predict_proba(self, X):\n",
    "        pred_proba = np.zeros([X.shape[0], 6])\n",
    "        pred = self.predict(X)\n",
    "        for i in range(pred.shape[0]):\n",
    "            pred_proba[i][int(pred[i])] = 1\n",
    "        return pred_proba\n",
    "\n",
    "class KNeighborsRegressorFixed(KNeighborsRegressor):\n",
    "    def predict_proba(self, X):\n",
    "        pred_proba = np.zeros([X.shape[0], 6])\n",
    "        pred = self.predict(X)\n",
    "        for i in range(pred.shape[0]):\n",
    "            pred_proba[i][int(pred[i])] = 1\n",
    "        return pred_proba\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    # Credits for the function vcorrcoef go to:\n",
    "    # https://waterprogramming.wordpress.com/2014/06/13/numpy-vectorized-correlation-coefficient/\n",
    "    def __vcorrcoef__(self, X, y):\n",
    "        Xm = np.ones(X.shape[1]) * np.reshape(np.mean(X, axis=1), (X.shape[0], 1))\n",
    "        ym = np.mean(y)\n",
    "        r_num = np.sum((X - Xm) * (y - ym), axis=1)\n",
    "        r_den = np.sqrt(np.sum((X - Xm) ** 2, axis=1) * np.sum((y - ym) ** 2))\n",
    "        r = r_num / r_den\n",
    "        return r\n",
    "\n",
    "    def __init__(self):\n",
    "        clf1 = OneVsOneClassifierFixed(MultinomialNB(), n_jobs=-1)\n",
    "        clf2 = OneVsOneClassifierFixed(RandomForestClassifier(random_state=234, class_weight=\"balanced\"), n_jobs=-1)\n",
    "        clf3 = KNeighborsRegressorFixed(n_neighbors=4, n_jobs=-1)\n",
    "        clf4 = OneVsOneClassifierFixed(GradientBoostingClassifier(random_state=725, loss='exponential'), n_jobs=-1)\n",
    "        clf5 = OneVsOneClassifierFixed(LinearSVC(loss='hinge', multi_class='crammer_singer'), n_jobs=-1)\n",
    "        # DON'T USE: not installed on the server\n",
    "        #clf6 = OneVsOneClassifierFixed(XGBClassifier(), n_jobs=-1)\n",
    "        #150: 0.372, 0.350, 0.369, 0.36, 0.371, 0368\n",
    "        self.clf = VotingClassifier(estimators=[('mnb', clf1), ('rf', clf2), ('knr', clf3), ('gb', clf4), ('lsvc', clf5)], voting='soft', weights=[1, 0, 0, 0, 1])\n",
    "        #self.clf = OneVsOneClassifierFixed(LinearSVC(loss='hinge', multi_class='crammer_singer', C=1.3), n_jobs=-1)\n",
    "        #self.clf = OneVsOneClassifier(RandomForestClassifier(class_weight=\"balanced\"), n_jobs=-1)\n",
    "        #self.clf = OneVsOneClassifier(MultinomialNB(), n_jobs=-1)\n",
    "        # 0.396\n",
    "        #OneVsOneClassifier(MultinomialNB(), n_jobs=-1)\n",
    "        # 0.392\n",
    "        #OneVsOneClassifier(RandomForestClassifier(random_state=234, class_weight=\"balanced\"), n_jobs=-1)\n",
    "        # 0.39\n",
    "        #KNeighborsRegressor(n_neighbors=4, n_jobs=-1)\n",
    "        # 0.387\n",
    "        #OneVsOneClassifier(GradientBoostingClassifier(random_state=725, loss='exponential'), n_jobs=-1)\n",
    "        \n",
    "        # 0.356\n",
    "        #OneVsOneClassifier(SGDClassifier(loss='modified_huber', tol=1e-3), n_jobs=-1)\n",
    "        # 0.352\n",
    "        #OneVsOneClassifier(MLPClassifier(activation='relu', solver='lbfgs', random_state=111), n_jobs=-1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.hstack([X, X ** 2])\n",
    "        correlations = self.__vcorrcoef__(X.T, y)\n",
    "        correlations_with_numbers = zip(correlations, range(len(correlations)))\n",
    "        correlations_with_numbers = sorted(correlations_with_numbers, key=lambda tup: abs(tup[0]), reverse=True)\n",
    "        self.features = sorted(map(lambda tup: tup[1], correlations_with_numbers[:20]))\n",
    "        self.clf.fit(X[:, self.features], y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.hstack([X, X ** 2])\n",
    "        return self.clf.predict(X[:, self.features].todense())\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.hstack([X, X ** 2])\n",
    "        pred_proba = np.zeros([X[:, self.features].shape[0], 6])\n",
    "        try:\n",
    "            pred_proba = self.clf.predict_proba(X[:, self.features])\n",
    "        except AttributeError:\n",
    "            pred = self.clf.predict(X[:, self.features])\n",
    "            for i in range(pred.shape[0]):\n",
    "                pred_proba[i][int(pred[i])] = 1\n",
    "        return pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local testing (before submission)\n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. For this we provide a unit test. Note that the test runs on your files in [`submissions/pawel_guzewicz`](/tree/submissions/pawel_guzewicz), not on the classes defined in the cells of this notebook.\n",
    "\n",
    "First `pip install ramp-workflow` or install it from the [github repo](https://github.com/paris-saclay-cds/ramp-workflow). Make sure that the python files `feature_extractor.py` and `classifier.py` are in the  [`submissions/pawel_guzewicz`](/tree/submissions/pawel_guzewicz) folder, and the data `train.csv` and `test.csv` are in [`data`](/tree/data). Then run\n",
    "\n",
    "```ramp_test_submission```\n",
    "\n",
    "If it runs and print training and test errors on each fold, then you can submit the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the small subset of training set (quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.326\n",
      "0.317\n",
      "0.337\n",
      "0.314\n",
      "0.376\n",
      "0.363\n",
      "0.262\n",
      "0.362\n",
      "0.313\n",
      "0.342\n",
      "\n",
      "0.331\n"
     ]
    }
   ],
   "source": [
    "runs = 10\n",
    "scores_sum = 0\n",
    "for i in range(runs):\n",
    "    score = !ramp_test_submission --quick-test --submission=pawel_guzewicz 2> /dev/null | grep test | tail -1 | cut -d' ' -f 4\n",
    "    score = score.s.split(\" \")[0][13:18]\n",
    "    if score[3] == '\\\\':\n",
    "        score = score[:3]\n",
    "    elif score[4] == '\\\\':\n",
    "        score = score[:4]\n",
    "    print(score)\n",
    "    scores_sum += float(score)\n",
    "print(\"\\n{0:.3f}\".format(scores_sum / runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = !ramp_test_submission --submission=pawel_guzewicz 2> /dev/null | grep test | tail -1 | cut -d' ' -f 4\n",
    "score = score.s.split(\" \")[0][13:18]\n",
    "if score[3] == '\\\\':\n",
    "    score = score[:3]\n",
    "else if score[4] == '\\\\':\n",
    "    score = score[:4]\n",
    "print(float(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to [ramp.studio](http://ramp.studio)\n",
    "\n",
    "Once you found a good feature extractor and classifier, you can submit them to [ramp.studio](http://www.ramp.studio). First, if it is your first time using RAMP, [sign up](http://www.ramp.studio/sign_up), otherwise [log in](http://www.ramp.studio/login). Then find an open event on the particular problem, for example, the event fake_news ([Saclay Datacamp](http://www.ramp.studio/events/fake_news_saclay_datacamp_17), [DataFest Tbilisi](https://www.ramp.studio/events/fake_news_tbilisi)) for this RAMP. Sign up for the event. Both signups are controled by RAMP administrators, so there **can be a delay between asking for signup and being able to submit**.\n",
    "\n",
    "Once your signup request is accepted, you can go to your sandbox ([Saclay Datacamp](http://www.ramp.studio/events/fake_news_saclay_datacamp_17/sandbox), [DataFest Tbilisi](https://www.ramp.studio/events/fake_news_tbilisi/sandbox)) and copy-paste (or upload) [`feature_extractor.py`](/edit/submissions/pawel_guzewicz/feature_extractor.py) and [`classifier.py`](/edit/submissions/pawel_guzewicz/classifier.py) from `submissions/pawel_guzewicz`. Save it, rename it, then submit it. The submission is trained and tested on our backend in the same way as `ramp_test_submission` does it locally. While your submission is waiting in the queue and being trained, you can find it in the \"New submissions (pending training)\" table in my submissions ([Saclay Datacamp](http://www.ramp.studio/events/fake_news_saclay_datacamp_17/my_submissions), [DataFest Tbilisi](https://www.ramp.studio/events/fake_news_tbilisi/my_submissions)). Once it is trained, you get a mail, and your submission shows up on the public leaderboard ([Saclay Datacamp](http://www.ramp.studio/events/fake_news_saclay_datacamp_17/leaderboard), [DataFest Tbilisi](https://www.ramp.studio/events/fake_news_tbilisi/leaderboard)). \n",
    "If there is an error (despite having tested your submission locally with `ramp_test_submission`), it will show up in the \"Failed submissions\" table in my submissions ([Saclay Datacamp](http://www.ramp.studio/events/fake_news_saclay_datacamp_17/my_submissions), [DataFest Tbilisi](https://www.ramp.studio/events/fake_news_tbilisi/my_submissions)). You can click on the error to see part of the trace.\n",
    "\n",
    "After submission, do not forget to give credits to the previous submissions you reused or integrated into your submission.\n",
    "\n",
    "The data set we use at the backend is usually different from what you find in the starting kit, so the score may be different.\n",
    "\n",
    "The usual way to work with RAMP is to explore solutions, add feature transformations, select models, perhaps do some AutoML/hyperopt, etc., _locally_, and checking them with `ramp_test_submission`. The script prints mean cross-validation scores \n",
    "```\n",
    "----------------------------\n",
    "train sacc = 0.77 ± 0.012\n",
    "train acc = 0.983 ± 0.01\n",
    "train tfacc = 0.835 ± 0.014\n",
    "valid sacc = 0.361 ± 0.05\n",
    "valid acc = 0.144 ± 0.119\n",
    "valid tfacc = 0.575 ± 0.101\n",
    "test sacc = 0.355 ± 0.013\n",
    "test acc = 0.197 ± 0.023\n",
    "test tfacc = 0.544 ± 0.021\n",
    "```\n",
    "The official score in this RAMP (the first score column after \"historical contributivity\" on the leader board ([Saclay Datacamp](http://www.ramp.studio/events/fake_news_saclay_datacamp_17/leaderboard), [DataFest Tbilisi](https://www.ramp.studio/events/fake_news_tbilisi/leaderboard)) is smoothed accuracy, so the line that is relevant in the output of `ramp_test_submission` is `valid sacc = 0.361 ± 0.05`. When the score is good enough, you can submit it at the RAMP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the [README](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/README.md) of the [ramp-workflow library](https://github.com/paris-saclay-cds/ramp-workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "Don't hesitate to [contact us](mailto:admin@ramp.studio?subject=fake news notebook)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
